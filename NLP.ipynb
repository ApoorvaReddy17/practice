{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdKo9UTwKvz3W0cEviGR6O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ApoorvaReddy17/practice/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLP**\n",
        "\n",
        "NLP is a field of AI that focuses on the interaction between computers and human language. It involves understanding, processing and generating natural language text or speech"
      ],
      "metadata": {
        "id": "e_JksywCL-Ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TOKENIZATION**\n",
        "\n",
        "Breaking text into smaller units (words,sentences, or subwords)"
      ],
      "metadata": {
        "id": "pMXFB2XtOovM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenization using nltk**"
      ],
      "metadata": {
        "id": "B5jL8vG8NTMt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90kekRl-9pGm",
        "outputId": "1ab6872a-2479-434e-b802-bf6fee319dcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ai = \"\"\"NLP is basic building block to build llm model. ChatGPT is an advanced language model developed by openAI, based on the GPT(Generative Pre-Tranformer) architecture. Specifically, ChatGPT belong to the series of GPT models,with GPT-4 being one of the latest versions. These models are designed to understand and generate human-like text based on the input they receive.\"\"\"\n",
        "ai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "icqHVTkI_Lba",
        "outputId": "ec1f2c60-5ebd-454c-a06e-17f5d1f92f60"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'NLP is basic building block to build llm model. ChatGPT is an advanced language model developed by openAI, based on the GPT(Generative Pre-Tranformer) architecture. Specifically, ChatGPT belong to the series of GPT models,with GPT-4 being one of the latest versions. These models are designed to understand and generate human-like text based on the input they receive.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(ai)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv8PbQ2lCxml",
        "outputId": "0586d01d-91c4-4d07-d1c2-c8e5dddd396f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sentence Tokens**\n",
        "\n",
        "Sentence segmentation\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n"
      ],
      "metadata": {
        "id": "Uw9BquIELp_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "s_tokens = sent_tokenize(ai)\n",
        "s_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyJlK9Tt_LWB",
        "outputId": "8fb7b783-240f-4106-f8e2-fb0ce3ee5b8a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP is basic building block to build llm model.',\n",
              " 'ChatGPT is an advanced language model developed by openAI, based on the GPT(Generative Pre-Tranformer) architecture.',\n",
              " 'Specifically, ChatGPT belong to the series of GPT models,with GPT-4 being one of the latest versions.',\n",
              " 'These models are designed to understand and generate human-like text based on the input they receive.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(s_tokens))\n",
        "len(s_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZ6yi0dl_Lpp",
        "outputId": "f8b9107b-662b-46ab-b173-44b7ad24ae7e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **WORD TOKENS**\n",
        "\n",
        "Simple word tokenization\n",
        "\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "UHNwLYpkMEuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "w_tokens = word_tokenize(ai)\n",
        "print(w_tokens)\n",
        "print(type(w_tokens))\n",
        "len(w_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCp0ydgo_Llm",
        "outputId": "618911b2-48be-4549-924c-71751b92a877"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLP', 'is', 'basic', 'building', 'block', 'to', 'build', 'llm', 'model', '.', 'ChatGPT', 'is', 'an', 'advanced', 'language', 'model', 'developed', 'by', 'openAI', ',', 'based', 'on', 'the', 'GPT', '(', 'Generative', 'Pre-Tranformer', ')', 'architecture', '.', 'Specifically', ',', 'ChatGPT', 'belong', 'to', 'the', 'series', 'of', 'GPT', 'models', ',', 'with', 'GPT-4', 'being', 'one', 'of', 'the', 'latest', 'versions', '.', 'These', 'models', 'are', 'designed', 'to', 'understand', 'and', 'generate', 'human-like', 'text', 'based', 'on', 'the', 'input', 'they', 'receive', '.']\n",
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TOKEN FREQUENCY**"
      ],
      "metadata": {
        "id": "cMrQsoR2PIx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "fdist = FreqDist(w_tokens)\n",
        "print(type(fdist))\n",
        "fdist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBKYqBEr_Lgj",
        "outputId": "dec14ffb-2eda-4560-de2b-80be3e6e5995"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'nltk.probability.FreqDist'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'.': 4, 'the': 4, 'to': 3, ',': 3, 'is': 2, 'model': 2, 'ChatGPT': 2, 'based': 2, 'on': 2, 'GPT': 2, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = FreqDist()\n",
        "for word in w_tokens:\n",
        "  f[word.lower()]+=1\n",
        "f"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX0CFr1Y_LRZ",
        "outputId": "65b29e8c-da9e-4b55-e2e3-7749e01b5bd2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'.': 4, 'the': 4, 'to': 3, ',': 3, 'is': 2, 'model': 2, 'chatgpt': 2, 'based': 2, 'on': 2, 'gpt': 2, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f['model']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0EgtiTj_LM6",
        "outputId": "d7556c27-a482-498d-8bd8-b19a3e1ee376"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f['chatgpt']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbfdJu-aRNVc",
        "outputId": "bb34d085-c287-45a8-bdff-08857a936540"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TOKENS TYPE**\n",
        "\n",
        "*   bigrams (2)\n",
        "*  trigrams (3)\n",
        "*  ngrams (3+)"
      ],
      "metadata": {
        "id": "fU1LvyNPRE9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import bigrams, trigrams, ngrams"
      ],
      "metadata": {
        "id": "y3Lwxwdp_LJd"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bigrams using word tokens\n",
        "b = list(nltk.bigrams(w_tokens))\n",
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez-ZxunaRmgi",
        "outputId": "40f5b1f1-e110-4497-e6b6-e1733eb563b3"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('NLP', 'is'),\n",
              " ('is', 'basic'),\n",
              " ('basic', 'building'),\n",
              " ('building', 'block'),\n",
              " ('block', 'to'),\n",
              " ('to', 'build'),\n",
              " ('build', 'llm'),\n",
              " ('llm', 'model'),\n",
              " ('model', '.'),\n",
              " ('.', 'ChatGPT'),\n",
              " ('ChatGPT', 'is'),\n",
              " ('is', 'an'),\n",
              " ('an', 'advanced'),\n",
              " ('advanced', 'language'),\n",
              " ('language', 'model'),\n",
              " ('model', 'developed'),\n",
              " ('developed', 'by'),\n",
              " ('by', 'openAI'),\n",
              " ('openAI', ','),\n",
              " (',', 'based'),\n",
              " ('based', 'on'),\n",
              " ('on', 'the'),\n",
              " ('the', 'GPT'),\n",
              " ('GPT', '('),\n",
              " ('(', 'Generative'),\n",
              " ('Generative', 'Pre-Tranformer'),\n",
              " ('Pre-Tranformer', ')'),\n",
              " (')', 'architecture'),\n",
              " ('architecture', '.'),\n",
              " ('.', 'Specifically'),\n",
              " ('Specifically', ','),\n",
              " (',', 'ChatGPT'),\n",
              " ('ChatGPT', 'belong'),\n",
              " ('belong', 'to'),\n",
              " ('to', 'the'),\n",
              " ('the', 'series'),\n",
              " ('series', 'of'),\n",
              " ('of', 'GPT'),\n",
              " ('GPT', 'models'),\n",
              " ('models', ','),\n",
              " (',', 'with'),\n",
              " ('with', 'GPT-4'),\n",
              " ('GPT-4', 'being'),\n",
              " ('being', 'one'),\n",
              " ('one', 'of'),\n",
              " ('of', 'the'),\n",
              " ('the', 'latest'),\n",
              " ('latest', 'versions'),\n",
              " ('versions', '.'),\n",
              " ('.', 'These'),\n",
              " ('These', 'models'),\n",
              " ('models', 'are'),\n",
              " ('are', 'designed'),\n",
              " ('designed', 'to'),\n",
              " ('to', 'understand'),\n",
              " ('understand', 'and'),\n",
              " ('and', 'generate'),\n",
              " ('generate', 'human-like'),\n",
              " ('human-like', 'text'),\n",
              " ('text', 'based'),\n",
              " ('based', 'on'),\n",
              " ('on', 'the'),\n",
              " ('the', 'input'),\n",
              " ('input', 'they'),\n",
              " ('they', 'receive'),\n",
              " ('receive', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bigrams using sentence\n",
        "sentence_bigrams = list(bigrams(s_tokens))\n",
        "sentence_bigrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeHM3gH-PVJH",
        "outputId": "cacc0121-720f-44ad-889c-8a9bca1e8694"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('NLP is basic building block to build llm model.',\n",
              "  'ChatGPT is an advanced language model developed by openAI, based on the GPT(Generative Pre-Tranformer) architecture.'),\n",
              " ('ChatGPT is an advanced language model developed by openAI, based on the GPT(Generative Pre-Tranformer) architecture.',\n",
              "  'Specifically, ChatGPT belong to the series of GPT models,with GPT-4 being one of the latest versions.'),\n",
              " ('Specifically, ChatGPT belong to the series of GPT models,with GPT-4 being one of the latest versions.',\n",
              "  'These models are designed to understand and generate human-like text based on the input they receive.')]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = list(nltk.trigrams(w_tokens))\n",
        "print(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFIJOBh3RmaA",
        "outputId": "21712116-fffb-40e7-c57e-2a0153edd6f6"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NLP', 'is', 'basic'), ('is', 'basic', 'building'), ('basic', 'building', 'block'), ('building', 'block', 'to'), ('block', 'to', 'build'), ('to', 'build', 'llm'), ('build', 'llm', 'model'), ('llm', 'model', '.'), ('model', '.', 'ChatGPT'), ('.', 'ChatGPT', 'is'), ('ChatGPT', 'is', 'an'), ('is', 'an', 'advanced'), ('an', 'advanced', 'language'), ('advanced', 'language', 'model'), ('language', 'model', 'developed'), ('model', 'developed', 'by'), ('developed', 'by', 'openAI'), ('by', 'openAI', ','), ('openAI', ',', 'based'), (',', 'based', 'on'), ('based', 'on', 'the'), ('on', 'the', 'GPT'), ('the', 'GPT', '('), ('GPT', '(', 'Generative'), ('(', 'Generative', 'Pre-Tranformer'), ('Generative', 'Pre-Tranformer', ')'), ('Pre-Tranformer', ')', 'architecture'), (')', 'architecture', '.'), ('architecture', '.', 'Specifically'), ('.', 'Specifically', ','), ('Specifically', ',', 'ChatGPT'), (',', 'ChatGPT', 'belong'), ('ChatGPT', 'belong', 'to'), ('belong', 'to', 'the'), ('to', 'the', 'series'), ('the', 'series', 'of'), ('series', 'of', 'GPT'), ('of', 'GPT', 'models'), ('GPT', 'models', ','), ('models', ',', 'with'), (',', 'with', 'GPT-4'), ('with', 'GPT-4', 'being'), ('GPT-4', 'being', 'one'), ('being', 'one', 'of'), ('one', 'of', 'the'), ('of', 'the', 'latest'), ('the', 'latest', 'versions'), ('latest', 'versions', '.'), ('versions', '.', 'These'), ('.', 'These', 'models'), ('These', 'models', 'are'), ('models', 'are', 'designed'), ('are', 'designed', 'to'), ('designed', 'to', 'understand'), ('to', 'understand', 'and'), ('understand', 'and', 'generate'), ('and', 'generate', 'human-like'), ('generate', 'human-like', 'text'), ('human-like', 'text', 'based'), ('text', 'based', 'on'), ('based', 'on', 'the'), ('on', 'the', 'input'), ('the', 'input', 'they'), ('input', 'they', 'receive'), ('they', 'receive', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = list(nltk.ngrams(w_tokens,5))\n",
        "print(n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDeJ2EpnRmTB",
        "outputId": "62dda8b8-3cb6-4091-e10b-2fe8ebc91cdc"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NLP', 'is', 'basic', 'building', 'block'), ('is', 'basic', 'building', 'block', 'to'), ('basic', 'building', 'block', 'to', 'build'), ('building', 'block', 'to', 'build', 'llm'), ('block', 'to', 'build', 'llm', 'model'), ('to', 'build', 'llm', 'model', '.'), ('build', 'llm', 'model', '.', 'ChatGPT'), ('llm', 'model', '.', 'ChatGPT', 'is'), ('model', '.', 'ChatGPT', 'is', 'an'), ('.', 'ChatGPT', 'is', 'an', 'advanced'), ('ChatGPT', 'is', 'an', 'advanced', 'language'), ('is', 'an', 'advanced', 'language', 'model'), ('an', 'advanced', 'language', 'model', 'developed'), ('advanced', 'language', 'model', 'developed', 'by'), ('language', 'model', 'developed', 'by', 'openAI'), ('model', 'developed', 'by', 'openAI', ','), ('developed', 'by', 'openAI', ',', 'based'), ('by', 'openAI', ',', 'based', 'on'), ('openAI', ',', 'based', 'on', 'the'), (',', 'based', 'on', 'the', 'GPT'), ('based', 'on', 'the', 'GPT', '('), ('on', 'the', 'GPT', '(', 'Generative'), ('the', 'GPT', '(', 'Generative', 'Pre-Tranformer'), ('GPT', '(', 'Generative', 'Pre-Tranformer', ')'), ('(', 'Generative', 'Pre-Tranformer', ')', 'architecture'), ('Generative', 'Pre-Tranformer', ')', 'architecture', '.'), ('Pre-Tranformer', ')', 'architecture', '.', 'Specifically'), (')', 'architecture', '.', 'Specifically', ','), ('architecture', '.', 'Specifically', ',', 'ChatGPT'), ('.', 'Specifically', ',', 'ChatGPT', 'belong'), ('Specifically', ',', 'ChatGPT', 'belong', 'to'), (',', 'ChatGPT', 'belong', 'to', 'the'), ('ChatGPT', 'belong', 'to', 'the', 'series'), ('belong', 'to', 'the', 'series', 'of'), ('to', 'the', 'series', 'of', 'GPT'), ('the', 'series', 'of', 'GPT', 'models'), ('series', 'of', 'GPT', 'models', ','), ('of', 'GPT', 'models', ',', 'with'), ('GPT', 'models', ',', 'with', 'GPT-4'), ('models', ',', 'with', 'GPT-4', 'being'), (',', 'with', 'GPT-4', 'being', 'one'), ('with', 'GPT-4', 'being', 'one', 'of'), ('GPT-4', 'being', 'one', 'of', 'the'), ('being', 'one', 'of', 'the', 'latest'), ('one', 'of', 'the', 'latest', 'versions'), ('of', 'the', 'latest', 'versions', '.'), ('the', 'latest', 'versions', '.', 'These'), ('latest', 'versions', '.', 'These', 'models'), ('versions', '.', 'These', 'models', 'are'), ('.', 'These', 'models', 'are', 'designed'), ('These', 'models', 'are', 'designed', 'to'), ('models', 'are', 'designed', 'to', 'understand'), ('are', 'designed', 'to', 'understand', 'and'), ('designed', 'to', 'understand', 'and', 'generate'), ('to', 'understand', 'and', 'generate', 'human-like'), ('understand', 'and', 'generate', 'human-like', 'text'), ('and', 'generate', 'human-like', 'text', 'based'), ('generate', 'human-like', 'text', 'based', 'on'), ('human-like', 'text', 'based', 'on', 'the'), ('text', 'based', 'on', 'the', 'input'), ('based', 'on', 'the', 'input', 'they'), ('on', 'the', 'input', 'they', 'receive'), ('the', 'input', 'they', 'receive', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ng = list(nltk.ngrams(w_tokens))\n",
        "# TypeError: ngrams() missing 1 required positional argument: 'n'"
      ],
      "metadata": {
        "id": "vJobDoxLRmNb"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenization Using spacy**\n",
        "\n",
        "Efficient NLP processing"
      ],
      "metadata": {
        "id": "36Fok-G1Nnhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "i5dxf8oORmIK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"Twitter CEO Elon Musk arrived at the Staples Center in Los Angeles, California.\""
      ],
      "metadata": {
        "id": "XpM-Pfj8_KqP"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc =nlp(sent)\n",
        "print(type(doc))\n",
        "doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2R1nQsZIRmC7",
        "outputId": "d64d4856-3071-4f6d-aa3f-c23da8557f97"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'spacy.tokens.doc.Doc'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Twitter CEO Elon Musk arrived at the Staples Center in Los Angeles,"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [token.text for token in doc]\n",
        "print(type(tokens))\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhAGKLkDRl-M",
        "outputId": "78f43286-6228-4505-a2a7-308a0bcadedf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "['Twitter', 'CEO', 'Elon', 'Musk', 'arrived', 'at', 'the', 'Staples', 'Center', 'in', 'Los', 'Angeles', ',']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Subword Tokenization (Byte-Pair Encoding - BPE)**\n",
        "\n",
        "used in deep learning models"
      ],
      "metadata": {
        "id": "PEnRU7R-O2JT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n"
      ],
      "metadata": {
        "id": "6zPcu8bDPO-v"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokens = t.tokenize('Twitter CEO Elon Musk arrived at the Staples Center in Los Angeles')\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcKTKbCUPOxz",
        "outputId": "75c6b796-42fc-4095-b560-5f8fc0f344e3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['twitter', 'ceo', 'el', '##on', 'mu', '##sk', 'arrived', 'at', 'the', 'staples', 'center', 'in', 'los', 'angeles']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(tokens),type(t))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAoCAVd8POkC",
        "outputId": "cec7d3bf-0038-4ffe-f564-b02fc2451355"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stemming**\n",
        "Stemming is a Natural Language Processing (NLP) technique used to reduce words to their root or base form. It removes suffixes and prefixes from words to obtain their \"stem,\" which helps in text processing tasks like search engines, sentiment analysis, and machine learning models.\n",
        "\n",
        "**Types of Stemming Algorithms**\n",
        "1. Porter Stemmer\n",
        "2. Snowball Stemmer\n",
        "3. Lancaster Stemmer"
      ],
      "metadata": {
        "id": "Od1zoGddTPjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Porter Stemmer**\n",
        "The Porter Stemmer is a widely used algorithm that applies a series of heuristic rules to remove common English suffixes.\n",
        "\n",
        " **The algorithm follows multiple steps:**\n",
        "*   Remove common plural suffixes →\n",
        "(\"caresses\" → \"caress\")\n",
        "*   Convert verb forms → (\"running\" → \"run\")\n",
        "*   Remove adverbial suffixes → (\"happily\" → \"happi\")\n",
        "\n",
        "### **Advantages**\n",
        "✔️ **Fast and widely used**\n",
        "\n",
        "✔️ Reduces words effectively\n",
        "### **Disadvantages**\n",
        "❌ Sometimes over-stems, producing incorrect stems\n",
        "\n",
        "❌ Cannot handle irregular words properly (e.g., \"went\" → \"went\")"
      ],
      "metadata": {
        "id": "CfC7S-JlXtF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "XzQSk49MPOdN"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a =  ['having',\"caresses\", \"running\", \"happily\", \"studies\", \"national\",\n",
        "          'maximum','effective',\"flying\"]\n",
        "print(a)\n",
        "lc = [ps.stem(word) for word in a]\n",
        "print(lc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PjQgvUKaJKM",
        "outputId": "727af42c-4659-4484-c54b-5f572951e809"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['having', 'caresses', 'running', 'happily', 'studies', 'national', 'maximum', 'effective', 'flying']\n",
            "['have', 'caress', 'run', 'happili', 'studi', 'nation', 'maximum', 'effect', 'fli']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in a:\n",
        "  print(word + ' : ' + ps.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vn4JLpG1eA3u",
        "outputId": "3ffa2c42-2cb9-40df-ea62-043e2a5faef0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "having : have\n",
            "caresses : caress\n",
            "running : run\n",
            "happily : happili\n",
            "studies : studi\n",
            "national : nation\n",
            "maximum : maximum\n",
            "effective : effect\n",
            "flying : fli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Snowball Stemmer (Porter2)**\n",
        "An improved version of the Porter Stemmer, the Snowball Stemmer supports multiple languages and offers better accuracy.\n",
        "\n",
        "### **Advantages:**\n",
        "✔️ More accurate than the Porter Stemmer\n",
        "\n",
        "✔️ Supports multiple languages\n",
        "### **Disadvantages:**\n",
        "❌ Slightly slower than Porter\n"
      ],
      "metadata": {
        "id": "H_R7vbnFZDWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "ss = SnowballStemmer('english')\n",
        "words =  ['having',\"caresses\", \"running\", \"happily\", \"studies\", \"national\",\n",
        "          'maximum','effective',\"flying\"]\n",
        "for word in words:\n",
        "      print(word + ' : ' + ss.stem(word))\n",
        "ss_words = [ss.stem(word) for word in words]\n",
        "print(ss_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rA3t4uUd-9d",
        "outputId": "9ce863a0-e8c7-4d95-a4c6-d0a4516c9021"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "having : have\n",
            "caresses : caress\n",
            "running : run\n",
            "happily : happili\n",
            "studies : studi\n",
            "national : nation\n",
            "maximum : maximum\n",
            "effective : effect\n",
            "flying : fli\n",
            "['have', 'caress', 'run', 'happili', 'studi', 'nation', 'maximum', 'effect', 'fli']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Lancaster Stemmer**\n",
        "The Lancaster Stemmer is more aggressive than the Porter Stemmer and reduces words more drastically.\n",
        "\n",
        "### **Advantages:**\n",
        "✔️ Very fast and efficient\n",
        "\n",
        "### **Disadvantages:**\n",
        "❌ Over-stemming leads to loss of meaning"
      ],
      "metadata": {
        "id": "250yARZsZEB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "ls = LancasterStemmer()\n",
        "words =  ['having',\"caresses\", \"running\", \"happily\", \"studies\", \"national\",\n",
        "          'maximum','effective',\"flying\"]\n",
        "r_word_ls = [[word, ls.stem(word)] for word in words]\n",
        "print(r_word_ls)\n",
        "print(type(r_word_ls))\n",
        "dict(r_word_ls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM7biforaI7x",
        "outputId": "f0a8d334-80b0-4bbc-8305-e716d49c69dd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['having', 'hav'], ['caresses', 'caress'], ['running', 'run'], ['happily', 'happy'], ['studies', 'study'], ['national', 'nat'], ['maximum', 'maxim'], ['effective', 'effect'], ['flying', 'fly']]\n",
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'having': 'hav',\n",
              " 'caresses': 'caress',\n",
              " 'running': 'run',\n",
              " 'happily': 'happy',\n",
              " 'studies': 'study',\n",
              " 'national': 'nat',\n",
              " 'maximum': 'maxim',\n",
              " 'effective': 'effect',\n",
              " 'flying': 'fly'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lemmatization**\n",
        "Lemmatization is an advanced Natural Language Processing (NLP) technique used to convert a word to its dictionary or base form (lemma) by considering its context and part of speech (POS). Unlike stemming, which just chops off suffixes, lemmatization ensures that the root word is a valid word in the language.\n"
      ],
      "metadata": {
        "id": "U3OaSC0rZF-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from  nltk.stem import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download WordNet data (if not already installed)\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXQfEEPlZMrr",
        "outputId": "44801a8f-c7bf-4ee3-82ee-017922faa68b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "wds =  ['having',\"caresses\", \"running\", \"happily\", \"studies\", \"national\",\n",
        "          'maximum','effective',\"flying\"]\n",
        "words_wl = [[word, lemmatizer.lemmatize(word)] for word in wds]\n",
        "words_wl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPD38pBiFBEU",
        "outputId": "6ab839dc-6f3c-41c1-897e-1b5264059705"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['having', 'having'],\n",
              " ['caresses', 'caress'],\n",
              " ['running', 'running'],\n",
              " ['happily', 'happily'],\n",
              " ['studies', 'study'],\n",
              " ['national', 'national'],\n",
              " ['maximum', 'maximum'],\n",
              " ['effective', 'effective'],\n",
              " ['flying', 'flying']]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "# Initialize WordNet Lemmatizer\n",
        "\n",
        "\n",
        "# Function to get lemma based on POS tagging\n",
        "def lemmatize_word(word, pos=wordnet.NOUN):\n",
        "    return lemmatizer.lemmatize(word, pos)\n",
        "\n",
        "# Test with different words\n",
        "words = [(\"running\", wordnet.VERB), (\"flies\", wordnet.NOUN),\n",
        " (\"better\", wordnet.ADJ), (\"studies\", wordnet.NOUN)]\n",
        "lemmas = [lemmatize_word(word, pos) for word, pos in words]\n",
        "\n",
        "print(lemmas)  # Output: ['run', 'fly', 'good', 'study']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxA-ObHLFBKs",
        "outputId": "d1de0045-e099-49e1-9b1b-d6cebf572e8a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'fly', 'good', 'study']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls = [[word,lemmatize_word(word, wordnet.VERB)] for word in wds]\n",
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGSmoMZl_Ltt",
        "outputId": "ec1792cd-3835-4ac3-dfeb-e66eca880ee5"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['having', 'have'],\n",
              " ['caresses', 'caress'],\n",
              " ['running', 'run'],\n",
              " ['happily', 'happily'],\n",
              " ['studies', 'study'],\n",
              " ['national', 'national'],\n",
              " ['maximum', 'maximum'],\n",
              " ['effective', 'effective'],\n",
              " ['flying', 'fly']]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lemmatization Using Spacy Liabrary**"
      ],
      "metadata": {
        "id": "lhWkNQ7j_5fO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "y3J-FWRzAHY7"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Spacy English language model\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "WGFMPFKRALDU"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"The children are running and playing happily.\"\n",
        "doc = nlp(sentence)\n",
        "ls = [ word.lemma_ for word in doc]\n",
        "print(\" \".join(ls))\n",
        "print(type(ls))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE8lv3GkAlyI",
        "outputId": "9dd65ebc-9896-4733-ef29-cdeb8b5aa704"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the child be run and play happily .\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to lemmatize sentence\n",
        "def lemmatize_text(text):\n",
        "    doc = nlp(text)\n",
        "    return \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "# Example text\n",
        "sentence = \"The children are running and playing happily.\"\n",
        "lemmatized_sentence = lemmatize_text(sentence)\n",
        "\n",
        "print(lemmatized_sentence)\n",
        "# Output: 'the child be run and play happily .'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxpPmZWv_L0r",
        "outputId": "dfcf44ca-6367-4e88-ec99-a0a77aaa8835"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the child be run and play happily .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **POS (Parts of Speech)**"
      ],
      "metadata": {
        "id": "zPJmvgE6IMh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4v29iaBKVJM",
        "outputId": "86c0b350-9937-4f56-90a5-a74feba0a985"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"Elon Musk arrived at the Staples Center in Los Angeles, California.\"\n",
        "sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zHU0fgwyIbAb",
        "outputId": "92fba5da-9d24-4ec3-bb14-34f6d30d9d1f"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Elon Musk arrived at the Staples Center in Los Angeles, California.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokens = word_tokenize(sent)\n",
        "sent_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ2mq61_Ia6V",
        "outputId": "20b67b73-2da0-4385-d197-0e73b427c2b7"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Elon',\n",
              " 'Musk',\n",
              " 'arrived',\n",
              " 'at',\n",
              " 'the',\n",
              " 'Staples',\n",
              " 'Center',\n",
              " 'in',\n",
              " 'Los',\n",
              " 'Angeles',\n",
              " ',',\n",
              " 'California',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in sent_tokens:\n",
        "  print(nltk.pos_tag([token]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKaq0tGSIazy",
        "outputId": "fba315be-44a2-4a1f-d501-8d3563f40c95"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Elon', 'NN')]\n",
            "[('Musk', 'NN')]\n",
            "[('arrived', 'VBN')]\n",
            "[('at', 'IN')]\n",
            "[('the', 'DT')]\n",
            "[('Staples', 'NNS')]\n",
            "[('Center', 'NNP')]\n",
            "[('in', 'IN')]\n",
            "[('Los', 'NNP')]\n",
            "[('Angeles', 'NNP')]\n",
            "[(',', ',')]\n",
            "[('California', 'NNP')]\n",
            "[('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZF4ig1TsIaoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2DCe61mOIaiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # **STOPWORDS**\n",
        " Stopwords are commonly used words (such as \"is,\" \"the,\" \"and,\" \"in,\" etc.) that do not carry significant meaning and are often removed from text data in Natural Language Processing (NLP) tasks to improve efficiency and accuracy."
      ],
      "metadata": {
        "id": "jpFkzIU3CZ09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ggopi1TrFA9L",
        "outputId": "cd417850-647d-490d-f14d-afce56b511dc"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5rlgJBsFA2x",
        "outputId": "e553fbc4-17c1-48f8-f515-b90328ff6919"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYevsdnfFAqc",
        "outputId": "a17f72ff-ece5-473f-eb9c-254cf7b90922"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "198"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(stopwords.words('french'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91SzKcYTFi-k",
        "outputId": "516aec21-fc7a-444c-b973-70f76fa05d41"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "157"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(stopwords.words('spanish'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XhViNQ2Fi3b",
        "outputId": "8d3524a4-6f77-4651-c70b-c1d87b0414d9"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "313"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(stopwords.words('german'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbjkFVXZFiwx",
        "outputId": "910bb8c1-af73-48b6-fbf7-5707cd6d8a75"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "232"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(stopwords.words('italian'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbnp27ZxF0ph",
        "outputId": "cf4a77f4-8bcb-49b8-bffc-97a256f9b9e3"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "279"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(stopwords.words('portuguese'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnjuWrBqF0h6",
        "outputId": "d4499fbe-052f-4b14-b921-34ae5fb6af64"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "207"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(stopwords.words('chinese'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vt4t4aubF7X2",
        "outputId": "a82a7aa8-f72b-48fd-b749-c26cc388b894"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "841"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I3Ur2IqhF7RS"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oksYHqMMF7Jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sFDs9pGCF7CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qcow34oXF0bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "21m_jBRiF0Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **stopwords using nltk**"
      ],
      "metadata": {
        "id": "pawdBj9kERCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download stopwords list\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Example text\n",
        "text = \"This is an example showing how to remove stopwords from a sentence.\"\n",
        "\n",
        "# Tokenize text\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "print(filtered_words)\n",
        "# Output: ['example', 'showing', 'remove', 'stopwords', 'sentence', '.']"
      ],
      "metadata": {
        "id": "Fz9rvQf6FAwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stopwords using spacy**"
      ],
      "metadata": {
        "id": "ooxUytH5ElDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example text\n",
        "text = \"This is an example showing how to remove stopwords from a sentence.\"\n",
        "\n",
        "# Process text with spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_words = [token.text for token in doc if not token.is_stop]\n",
        "\n",
        "print(filtered_words)\n",
        "# Output: ['example', 'showing', 'remove', 'stopwords', 'sentence', '.']\n"
      ],
      "metadata": {
        "id": "6zc-H2bdFAke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eYXOClcRFAeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ydjcywPLFAXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FM1jBgBH_Khr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LzWVBxMk_Kbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tXI9CZpg-n1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zMXzrN91-n6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hDF7JbEP-oBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oEqZstGv-oEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jC2H4epi-oHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ('Python llm python genai llm Python llm Matplotlib llm Seaborn Network')"
      ],
      "metadata": {
        "id": "vzOx54VV_KmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"a = '''Artificial Intelligence refers to the intelligence of machines. This is in contrast to the natural intelligence of\n",
        "humans and animals. With Artificial Intelligence, machines perform functions such as learning, planning, reasoning and\n",
        "problem-solving. Most noteworthy, Artificial Intelligence is the simulation of human intelligence by machines.\n",
        "It is probably the fastest-growing development in the World of technology and innovation. Furthermore, many experts believe\n",
        "AI could solve major challenges and crisis situations.'''\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "X70oIelF_MCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n",
        "               the world have come and invaded us, captured our lands, conquered our minds.\n",
        "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
        "               the French, the Dutch, all of them came and looted us, took over what was ours.\n",
        "               Yet we have not done this to any other nation. We have not conquered anyone.\n",
        "               We have not grabbed their land, their culture,\n",
        "               their history and tried to enforce our way of life on them.\n",
        "               Why? Because we respect the freedom of others.That is why my\n",
        "               first vision is that of freedom. I believe that India got its first vision of\n",
        "               this in 1857, when we started the War of Independence. It is this freedom that\n",
        "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
        "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
        "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
        "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
        "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
        "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
        "               I have a third vision. India must stand up to the world. Because I believe that unless India\n",
        "               stands up to the world, no one will respect us. Only strength respects strength. We must be\n",
        "               strong not only as a military power but also as an economic power. Both must go hand-in-hand.\n",
        "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of\n",
        "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
        "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
        "               I see four milestones in my career. india is great compare to other country \"\"\"\n",
        "\n",
        "paragraph\n"
      ],
      "metadata": {
        "id": "OmggDCuW_L9x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}